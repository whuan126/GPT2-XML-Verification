{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 11:08:31.262803: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-28 11:08:31.262851: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-28 11:08:31.310351: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-28 11:08:31.403084: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-28 11:08:32.235542: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/whuan126/WJT_CS182_Project/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load pre-trained GPT2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id = tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Provide example xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_xml = \"\"\"\n",
    "    <car>\n",
    "        <make>\n",
    "        <model>\n",
    "        <year>\n",
    "        </year>\n",
    "        </model>\n",
    "        </make>\n",
    "    </car>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Design the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A new xml schedule: \\n <schedule>\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate the text and XML content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whuan126/WJT_CS182_Project/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/whuan126/WJT_CS182_Project/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['tempature'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/whuan126/WJT_CS182_Project/generateXML.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/whuan126/WJT_CS182_Project/generateXML.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m generated_text \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate( \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/whuan126/WJT_CS182_Project/generateXML.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     tokenizer\u001b[39m.\u001b[39;49mencode(prompt, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/whuan126/WJT_CS182_Project/generateXML.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/whuan126/WJT_CS182_Project/generateXML.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     num_beams\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/whuan126/WJT_CS182_Project/generateXML.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     no_repeat_ngram_size\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/whuan126/WJT_CS182_Project/generateXML.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     top_k \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/whuan126/WJT_CS182_Project/generateXML.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     top_p \u001b[39m=\u001b[39;49m \u001b[39m0.95\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/whuan126/WJT_CS182_Project/generateXML.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     tempature \u001b[39m=\u001b[39;49m \u001b[39m0.7\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/whuan126/WJT_CS182_Project/generateXML.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/whuan126/WJT_CS182_Project/generateXML.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m )\n",
      "File \u001b[0;32m~/WJT_CS182_Project/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/WJT_CS182_Project/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1485\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m model_kwargs \u001b[39m=\u001b[39m generation_config\u001b[39m.\u001b[39mupdate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m generation_config\u001b[39m.\u001b[39mvalidate()\n\u001b[0;32m-> 1485\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_model_kwargs(model_kwargs\u001b[39m.\u001b[39;49mcopy())\n\u001b[1;32m   1487\u001b[0m \u001b[39m# 2. Set generation parameters if not already defined\u001b[39;00m\n\u001b[1;32m   1488\u001b[0m logits_processor \u001b[39m=\u001b[39m logits_processor \u001b[39mif\u001b[39;00m logits_processor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m LogitsProcessorList()\n",
      "File \u001b[0;32m~/WJT_CS182_Project/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1262\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1259\u001b[0m         unused_model_args\u001b[39m.\u001b[39mappend(key)\n\u001b[1;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1262\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1263\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[39m{\u001b[39;00munused_model_args\u001b[39m}\u001b[39;00m\u001b[39m (note: typos in the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1264\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m generate arguments will also show up in this list)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1265\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['tempature'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "generated_text = model.generate( \n",
    "    tokenizer.encode(prompt, return_tensors=\"pt\"),\n",
    "    max_length=100,\n",
    "    num_beams=3,\n",
    "    no_repeat_ngram_size=3,\n",
    "    top_k = None,\n",
    "    top_p = 0.95,\n",
    "    tempature = 0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* input_ids: This is the tensor containing the input sequence.\n",
    "\n",
    "* max_length: The maximum length of the generated sequence.\n",
    "\n",
    "* num_beams: Number of \"beams\" for beam search. Beam search is a technique used in sequence generatio to explore multiple possible sequences concurrently. This can help improve the quality of the generated text.\n",
    "\n",
    "* no_repeat_ngram_size: Size of n-grams that should not be repeated. This helps the model avoid repetitive patterns in generated text\n",
    "\n",
    "* top_k: Number of highest probability vocabulary. This limits the nunber of tokens considered during sampling.\n",
    "\n",
    "* top_p: Cumulative probability threshold for nucleus sampling. This is an alternative to top-k sampling. The difference is it selects the next token based on the smallest set of tokens which the cumulative probability exceeds a specified value.\n",
    "\n",
    "* temperature: Used for controlling randomness in sampling. Higher temperature (closer to 1.0) makes the sampling random.\n",
    "\n",
    "* pad_token_id: ID of the padding token in vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Decode the generated text and XML content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_xml = tokenizer.decode(generated_text[0], skip_special_tokens = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate an xml file to represent a calendar.\n",
      "\n",
      "Example:\n",
      "\n",
      "<? xml version = \" 1.0 \" encoding = \" utf-8 \"?> < xmlns = \" http://www.w3.org/2001/XMLSchema-instance \" xmlns:xsi = \"http://schemas.microsoft.com/winfx/2006/xhtml\" xsi:schemaLocation = \"C:\\Program Files (x86)\\Microsoft\n"
     ]
    }
   ],
   "source": [
    "print(generated_xml)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
